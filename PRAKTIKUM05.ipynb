{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMD9AkMuN5QWKHcVU5UK/BP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luzyi0/2025-Vision_Computer_TI2B/blob/main/PRAKTIKUM05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Persiapan Environment"
      ],
      "metadata": {
        "id": "yFLCs_PS9mBH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhjpM8cL9eE8",
        "outputId": "dd53f38d-d59c-4393-f913-b9d7c5c0fd9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA Available: False\n",
            "CUDA Device: No GPU\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
        "print(f\"CUDA Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'No GPU'}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install PyTorch dan torchvision\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "\n",
        "# Install library pendukung\n",
        "!pip install opencv-python matplotlib pillow numpy\n",
        "!pip install pycocotools  # Untuk evaluasi dengan COCO metrics\n",
        "\n",
        "# Install library untuk selective search (R-CNN dan Fast R-CNN)\n",
        "!pip install selectivesearch\n",
        "\n",
        "# Verify installation\n",
        "import torch\n",
        "import torchvision\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(f\"PyTorch Version: {torch.__version__}\")\n",
        "print(f\"Torchvision Version: {torchvision.__version__}\")\n",
        "print(f\"OpenCV Version: {cv2.__version__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOthtdfU9k2u",
        "outputId": "97c9153e-cfe3-4b37-8dc1-c4862d8a55c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.12/dist-packages (2.0.10)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from pycocotools) (2.0.2)\n",
            "Requirement already satisfied: selectivesearch in /usr/local/lib/python3.12/dist-packages (0.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from selectivesearch) (2.0.2)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.12/dist-packages (from selectivesearch) (0.25.2)\n",
            "Requirement already satisfied: scipy>=1.11.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image->selectivesearch) (1.16.3)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from scikit-image->selectivesearch) (3.5)\n",
            "Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.12/dist-packages (from scikit-image->selectivesearch) (11.3.0)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image->selectivesearch) (2.37.2)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image->selectivesearch) (2025.10.16)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.12/dist-packages (from scikit-image->selectivesearch) (25.0)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image->selectivesearch) (0.4)\n",
            "PyTorch Version: 2.8.0+cu126\n",
            "Torchvision Version: 0.23.0+cu126\n",
            "OpenCV Version: 4.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download dan extract dataset\n",
        "!wget https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip\n",
        "!unzip -q PennFudanPed.zip\n",
        "!ls PennFudanPed/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3WJVL4D-1gM",
        "outputId": "18c00c50-d193-4d63-8946-fffef0c928a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-21 01:28:27--  https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip\n",
            "Resolving www.cis.upenn.edu (www.cis.upenn.edu)... 158.130.69.163, 2607:f470:8:64:5ea5::d\n",
            "Connecting to www.cis.upenn.edu (www.cis.upenn.edu)|158.130.69.163|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 53723336 (51M) [application/zip]\n",
            "Saving to: ‘PennFudanPed.zip’\n",
            "\n",
            "PennFudanPed.zip    100%[===================>]  51.23M  64.3MB/s    in 0.8s    \n",
            "\n",
            "2025-11-21 01:28:28 (64.3 MB/s) - ‘PennFudanPed.zip’ saved [53723336/53723336]\n",
            "\n",
            "added-object-list.txt  Annotation  PedMasks  PNGImages\treadme.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import os\n",
        "import time\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--DiDJY5Btm3",
        "outputId": "bb7e546d-2216-405b-e31c-2b858ce2fd56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Praktikum"
      ],
      "metadata": {
        "id": "9pNFtNJMB4fj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Praktikum 1: Implementasi R-CNN"
      ],
      "metadata": {
        "id": "Zj0AzjQqCrWW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Langkah 1.1: Implementasi Sselective Search"
      ],
      "metadata": {
        "id": "7wJkwK9hCwmU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import selectivesearch\n",
        "\n",
        "def generate_region_proposals(image_path, scale=500, sigma=0.9, min_size=10):\n",
        "    \"\"\"\n",
        "    Generate region proposals using Selective Search\n",
        "    Args:\n",
        "        image_path: Path to input image\n",
        "        scale: Larger scale -> fewer regions\n",
        "        sigma: Width of Gaussian kernel for preprocessing\n",
        "        min_size: Minimum component size\n",
        "    Returns:\n",
        "        regions: List of proposed regions with coordinates\n",
        "    \"\"\"\n",
        "\n",
        "    # Load image\n",
        "    img = cv2.imread(image_path)\n",
        "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Run selective search\n",
        "    img_lbl, regions = selectivesearch.selective_search(\n",
        "        img_rgb, scale=scale, sigma=sigma, min_size=min_size\n",
        "    )\n",
        "\n",
        "    # Extract bounding boxes\n",
        "    candidates = []\n",
        "    for r in regions:\n",
        "        # Exclude regions with size < 2000 pixels\n",
        "        if r['size'] < 2000:\n",
        "            continue\n",
        "\n",
        "        # Get bounding box\n",
        "        x, y, w, h = r['rect']\n",
        "\n",
        "        # Exclude small regions\n",
        "        if w < 20 or h < 20:\n",
        "            continue\n",
        "\n",
        "        candidates.append({\n",
        "            'rect': (x, y, w, h),\n",
        "            'size': r['size']\n",
        "        })\n",
        "\n",
        "    # Sort by size and take top 2000\n",
        "    candidates = sorted(\n",
        "        candidates,\n",
        "        key=lambda x: x['size'],\n",
        "        reverse=True\n",
        "    )[:2000]\n",
        "\n",
        "    return candidates, img_rgb\n",
        "\n",
        "\n",
        "# Test selective search\n",
        "image_path = 'PennFudanPed/PNGImages/FudanPed00001.png'\n",
        "proposals, img = generate_region_proposals(image_path)\n",
        "print(f\"Generated {len(proposals)} region proposals\")\n",
        "\n",
        "# Visualize some proposals\n",
        "fig, ax = plt.subplots(1, figsize=(12, 8))\n",
        "ax.imshow(img)\n",
        "\n",
        "for i, prop in enumerate(proposals[:20]):  # Show first 20 proposals\n",
        "    x, y, w, h = prop['rect']\n",
        "    rect = patches.Rectangle(\n",
        "        (x, y), w, h,\n",
        "        linewidth=1,\n",
        "        edgecolor='r',\n",
        "        facecolor='none'\n",
        "    )\n",
        "    ax.add_patch(rect)\n",
        "\n",
        "ax.set_title(f'Top 20 Region Proposals (Total: {len(proposals)})')\n",
        "plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.savefig('rcnn_proposals.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "10XXtTi8B10P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Langkah 1.2: Feature Extraction dengan Pre-trained CNN"
      ],
      "metadata": {
        "id": "JVaR8FNHctI0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "\n",
        "class RCNNFeatureExtractor:\n",
        "    \"\"\"\n",
        "    Feature extractor for R-CNN using pre-trained ResNet50\n",
        "    \"\"\"\n",
        "    def __init__(self, device='cuda'):\n",
        "        self.device = device\n",
        "\n",
        "        # Load pre-trained ResNet50\n",
        "        weights = ResNet50_Weights.IMAGENET1K_V1\n",
        "        self.model = resnet50(weights=weights)\n",
        "\n",
        "        # Remove final classification layer\n",
        "        self.model = nn.Sequential(*list(self.model.children())[:-1])\n",
        "        self.model.to(device)\n",
        "        self.model.eval()\n",
        "\n",
        "        # Define transforms\n",
        "        self.transform = weights.transforms()\n",
        "\n",
        "    def extract_features(self, image, regions):\n",
        "        \"\"\"\n",
        "        Extract features from regions\n",
        "\n",
        "        Args:\n",
        "            image: PIL Image or numpy array\n",
        "            regions: List of region proposals\n",
        "\n",
        "        Returns:\n",
        "            features: Tensor of shape [N, 2048] where N is number of regions\n",
        "            valid_regions: List of valid regions (some may be filtered)\n",
        "        \"\"\"\n",
        "\n",
        "        if isinstance(image, np.ndarray):\n",
        "            image = Image.fromarray(image)\n",
        "\n",
        "        features_list = []\n",
        "        valid_regions = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for region in regions:\n",
        "                x, y, w, h = region['rect']\n",
        "\n",
        "                # Crop region\n",
        "                try:\n",
        "                    region_img = image.crop((x, y, x+w, y+h))\n",
        "\n",
        "                    # Transform and add batch dimension\n",
        "                    region_tensor = self.transform(region_img).unsqueeze(0).to(self.device)\n",
        "\n",
        "                    # Extract features\n",
        "                    features = self.model(region_tensor)\n",
        "                    features = features.squeeze()\n",
        "\n",
        "                    features_list.append(features.cpu())\n",
        "                    valid_regions.append(region)\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "        if len(features_list) == 0:\n",
        "            return None, []\n",
        "\n",
        "        features = torch.stack(features_list)\n",
        "        return features, valid_regions\n",
        "\n",
        "\n",
        "# Test feature extraction\n",
        "extractor = RCNNFeatureExtractor(device=device)\n",
        "features, valid_regions = extractor.extract_features(img, proposals[:100])\n",
        "print(f\"Extracted features shape: {features.shape}\")\n",
        "print(f\"Valid regions: {len(valid_regions)}\")"
      ],
      "metadata": {
        "id": "GDgaLflpeaEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Langkah 1.3: Classification dengan SVM"
      ],
      "metadata": {
        "id": "mpWbfEUYc1KV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "class RCNNClassifier:\n",
        "    \"\"\"\n",
        "    SVM classifier for R-CNN\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=2):\n",
        "        self.num_classes = num_classes\n",
        "        self.scaler = StandardScaler()\n",
        "        self.svm = SVC(kernel='linear', probability=True, random_state=42)\n",
        "\n",
        "    def train(self, features, labels):\n",
        "        \"\"\"\n",
        "        Train SVM classifier\n",
        "\n",
        "        Args:\n",
        "            features: numpy array of shape [N, feature_dim]\n",
        "            labels: numpy array of shape [N]\n",
        "        \"\"\"\n",
        "\n",
        "        # Normalize features\n",
        "        features_scaled = self.scaler.fit_transform(features)\n",
        "\n",
        "        # Train SVM\n",
        "        self.svm.fit(features_scaled, labels)\n",
        "        print(f\"SVM trained with {len(features)} samples\")\n",
        "\n",
        "    def predict(self, features):\n",
        "        \"\"\"\n",
        "        Predict class labels\n",
        "\n",
        "        Args:\n",
        "            features: numpy array of shape [N, feature_dim]\n",
        "\n",
        "        Returns:\n",
        "            predictions: class labels\n",
        "            probabilities: class probabilities\n",
        "        \"\"\"\n",
        "\n",
        "        features_scaled = self.scaler.transform(features)\n",
        "        predictions = self.svm.predict(features_scaled)\n",
        "        probabilities = self.svm.predict_proba(features_scaled)\n",
        "        return predictions, probabilities\n",
        "\n",
        "\n",
        "# Note: Training SVM requires labeled data\n",
        "# For demonstration, we'll use pre-trained Faster R-CNN instead\n",
        "print(\"Note: Full R-CNN training requires labeled region proposals\")\n",
        "print(\"For practical purposes, we'll demonstrate inference using Faster R-CNN\")"
      ],
      "metadata": {
        "id": "pTlW4M2teyLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Langkah 1.4: Complete R-CNN Pipeline (Simplified)"
      ],
      "metadata": {
        "id": "8EO9KNZcc6Sh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rcnn_inference_simplified(image_path, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Simplified R-CNN inference using selective search + pre-trained classifier\n",
        "\n",
        "    Args:\n",
        "        image_path: Path to input image\n",
        "        threshold: Confidence threshold\n",
        "\n",
        "    Returns:\n",
        "        detections: List of detected objects with boxes and scores\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Generate region proposals\n",
        "    print(\"Step 1: Generating region proposals...\")\n",
        "    proposals, img = generate_region_proposals(image_path, scale=500)\n",
        "    print(f\"Generated {len(proposals)} proposals\")\n",
        "\n",
        "    # Step 2: Extract features\n",
        "    print(\"Step 2: Extracting features from regions...\")\n",
        "    extractor = RCNNFeatureExtractor(device=device)\n",
        "    features, valid_regions = extractor.extract_features(img, proposals[:500])\n",
        "    print(f\"Extracted features from {len(valid_regions)} regions\")\n",
        "\n",
        "    # Step 3: Classification (using pre-trained Faster R-CNN for demo)\n",
        "    print(\"Step 3: Classifying regions...\")\n",
        "\n",
        "    # Load pre-trained Faster R-CNN for classification\n",
        "    model = fasterrcnn_resnet50_fpn(weights='DEFAULT')\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Convert image to tensor\n",
        "    transform = transforms.Compose([transforms.ToTensor()])\n",
        "    img_pil = Image.fromarray(img)\n",
        "    img_tensor = transform(img_pil).to(device)\n",
        "\n",
        "    # Get predictions\n",
        "    with torch.no_grad():\n",
        "        predictions = model([img_tensor])\n",
        "\n",
        "    # Filter predictions\n",
        "    boxes = predictions[0]['boxes'].cpu().numpy()\n",
        "    labels = predictions[0]['labels'].cpu().numpy()\n",
        "    scores = predictions[0]['scores'].cpu().numpy()\n",
        "\n",
        "    # Apply threshold\n",
        "    keep = scores > threshold\n",
        "    boxes = boxes[keep]\n",
        "    labels = labels[keep]\n",
        "    scores = scores[keep]\n",
        "\n",
        "    print(f\"Step 4: Found {len(boxes)} objects above threshold {threshold}\")\n",
        "\n",
        "    return boxes, labels, scores, img\n",
        "\n",
        "\n",
        "# Run R-CNN inference\n",
        "boxes, labels, scores, img = rcnn_inference_simplified(\n",
        "    'PennFudanPed/PNGImages/FudanPed00001.png',\n",
        "    threshold=0.7\n",
        ")\n",
        "\n",
        "# Visualize results\n",
        "COCO_CLASSES = [\n",
        "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane',\n",
        "    'bus', 'train', 'truck', 'boat', 'traffic light'\n",
        "]\n",
        "\n",
        "fig, ax = plt.subplots(1, figsize=(12, 8))\n",
        "ax.imshow(img)\n",
        "\n",
        "for box, label, score in zip(boxes, labels, scores):\n",
        "    x1, y1, x2, y2 = box\n",
        "    w, h = x2 - x1, y2 - y1\n",
        "\n",
        "    rect = patches.Rectangle(\n",
        "        (x1, y1),\n",
        "        w, h,\n",
        "        linewidth=2,\n",
        "        edgecolor='green',\n",
        "        facecolor='none'\n",
        "    )\n",
        "    ax.add_patch(rect)\n",
        "\n",
        "    class_name = COCO_CLASSES[label] if label < len(COCO_CLASSES) else f'class_{label}'\n",
        "    ax.text(\n",
        "        x1, y1-5,\n",
        "        f'{class_name}: {score:.2f}',\n",
        "        bbox=dict(facecolor='green', alpha=0.5),\n",
        "        fontsize=10,\n",
        "        color='white'\n",
        "    )\n",
        "\n",
        "ax.set_title('R-CNN Detection Results')\n",
        "plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.savefig('rcnn_results.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nDetection Summary:\")\n",
        "print(f\"Total detections: {len(boxes)}\")\n",
        "for i, (label, score) in enumerate(zip(labels, scores)):\n",
        "    class_name = COCO_CLASSES[label] if label < len(COCO_CLASSES) else f'class_{label}'\n",
        "    print(f\"  {i+1}. {class_name}: {score:.3f}\")"
      ],
      "metadata": {
        "id": "UXox0kPPe5kL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Praktikum 2: Implementasi Fast R-CNN"
      ],
      "metadata": {
        "id": "c1a3pylndFKg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Langkah 2.1: Implementasi Rol Pooling"
      ],
      "metadata": {
        "id": "XOs8YyWvdLvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.ops as ops\n",
        "\n",
        "class FastRCNNRoIPooling:\n",
        "    \"\"\"\n",
        "    RoI Pooling implementation for Fast R-CNN\n",
        "    \"\"\"\n",
        "    def __init__(self, output_size=(7, 7), spatial_scale=1.0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            output_size: Fixed output size (height, width)\n",
        "            spatial_scale: Scale factor from image to feature map\n",
        "        \"\"\"\n",
        "        self.output_size = output_size\n",
        "        self.spatial_scale = spatial_scale\n",
        "\n",
        "    def forward(self, features, boxes):\n",
        "        \"\"\"\n",
        "        Apply RoI Pooling\n",
        "\n",
        "        Args:\n",
        "            features: Feature map tensor [1, C, H, W]\n",
        "            boxes: Region proposals [N, 4] in format [x1, y1, x2, y2]\n",
        "\n",
        "        Returns:\n",
        "            pooled_features: Tensor [N, C, output_h, output_w]\n",
        "        \"\"\"\n",
        "\n",
        "        # Convert boxes to torchvision format\n",
        "        boxes_tensor = torch.FloatTensor(boxes).to(features.device)\n",
        "\n",
        "        # Add batch index (all boxes from same image)\n",
        "        batch_indices = torch.zeros(\n",
        "            (boxes_tensor.shape[0], 1),\n",
        "            dtype=boxes_tensor.dtype,\n",
        "            device=boxes_tensor.device\n",
        "        )\n",
        "\n",
        "        rois = torch.cat([batch_indices, boxes_tensor], dim=1)\n",
        "\n",
        "        # Apply RoI Pooling\n",
        "        pooled = ops.roi_pool(\n",
        "            features,\n",
        "            rois,\n",
        "            output_size=self.output_size,\n",
        "            spatial_scale=self.spatial_scale\n",
        "        )\n",
        "\n",
        "        return pooled\n",
        "\n",
        "\n",
        "# Test RoI Pooling\n",
        "def test_roi_pooling():\n",
        "    # Create dummy feature map\n",
        "    feature_map = torch.randn(1, 512, 50, 50).to(device)\n",
        "\n",
        "    # Create dummy boxes\n",
        "    boxes = np.array([\n",
        "        [10, 10, 100, 100],\n",
        "        [150, 150, 300, 300],\n",
        "        [200, 50, 400, 200]\n",
        "    ])\n",
        "\n",
        "    # Apply RoI Pooling\n",
        "    roi_pooling = FastRCNNRoIPooling(output_size=(7, 7), spatial_scale=1/16)\n",
        "    pooled_features = roi_pooling.forward(feature_map, boxes)\n",
        "\n",
        "    print(f\"Feature map shape: {feature_map.shape}\")\n",
        "    print(f\"Number of boxes: {len(boxes)}\")\n",
        "    print(f\"Pooled features shape: {pooled_features.shape}\")\n",
        "    print(f\"Each region now has fixed size: {pooled_features.shape[2:]}\")\n",
        "\n",
        "test_roi_pooling()"
      ],
      "metadata": {
        "id": "idc2O7VyfKtR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Langkah 2.2: Implementasi Fast R-CNN Model"
      ],
      "metadata": {
        "id": "4xmYPU_XdQP6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FastRCNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Fast R-CNN implementation using ResNet50 backbone\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=91, roi_output_size=(7, 7)):\n",
        "        super(FastRCNN, self).__init__()\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "        self.roi_output_size = roi_output_size\n",
        "\n",
        "        # Backbone: ResNet50 without final layers\n",
        "        resnet = torchvision.models.resnet50(weights='IMAGENET1K_V1')\n",
        "        self.backbone = nn.Sequential(*list(resnet.children())[:-2])\n",
        "\n",
        "        # Calculate spatial scale (image size / feature map size)\n",
        "        # For ResNet50, stride = 32\n",
        "        self.spatial_scale = 1.0 / 32.0\n",
        "\n",
        "        # Feature dimension after RoI pooling\n",
        "        feature_dim = 2048 * roi_output_size[0] * roi_output_size[1]\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(feature_dim, 4096)\n",
        "        self.fc2 = nn.Linear(4096, 4096)\n",
        "\n",
        "        # Classification head\n",
        "        self.cls_score = nn.Linear(4096, num_classes)\n",
        "\n",
        "        # Bounding box regression head\n",
        "        self.bbox_pred = nn.Linear(4096, num_classes * 4)\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, images, boxes):\n",
        "        \"\"\"\n",
        "        Forward pass\n",
        "\n",
        "        Args:\n",
        "            images: Tensor [B, 3, H, W]\n",
        "            boxes: List of region proposals for each image\n",
        "\n",
        "        Returns:\n",
        "            cls_scores: Classification scores [N, num_classes]\n",
        "            bbox_deltas: Bounding box regression [N, num_classes*4]\n",
        "        \"\"\"\n",
        "\n",
        "        # Extract features from entire image\n",
        "        features = self.backbone(images)\n",
        "\n",
        "        # Apply RoI Pooling\n",
        "        pooled_features = ops.roi_pool(\n",
        "            features,\n",
        "            boxes,\n",
        "            output_size=self.roi_output_size,\n",
        "            spatial_scale=self.spatial_scale\n",
        "        )\n",
        "\n",
        "        # Flatten\n",
        "        pooled_features = pooled_features.view(pooled_features.size(0), -1)\n",
        "\n",
        "        # Fully connected layers\n",
        "        x = self.relu(self.fc1(pooled_features))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Classification and regression\n",
        "        cls_scores = self.cls_score(x)\n",
        "        bbox_deltas = self.bbox_pred(x)\n",
        "\n",
        "        return cls_scores, bbox_deltas\n",
        "\n",
        "\n",
        "# Initialize model\n",
        "fast_rcnn_model = FastRCNN(num_classes=91).to(device)\n",
        "\n",
        "print(\"Fast R-CNN model initialized\")\n",
        "print(\n",
        "f\"Total parameters: {sum(p.numel() for p in fast_rcnn_model.parameters()):,}\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "7jCIXNtGfhDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Langkah 2.3: Fast R-CNN Inference"
      ],
      "metadata": {
        "id": "iPIEq9hEdVm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fast_rcnn_inference(image_path, model, threshold=0.7, nms_threshold=0.3):\n",
        "    \"\"\"\n",
        "    Fast R-CNN inference pipeline\n",
        "\n",
        "    Args:\n",
        "        image_path: Path to input image\n",
        "        model: Fast R-CNN model\n",
        "        threshold: Confidence threshold\n",
        "        nms_threshold: NMS IoU threshold\n",
        "\n",
        "    Returns:\n",
        "        boxes, labels, scores: Detection results\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Load and preprocess image\n",
        "    img = cv2.imread(image_path)\n",
        "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    img_pil = Image.fromarray(img_rgb)\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                             std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    img_tensor = transform(img_pil).unsqueeze(0).to(device)\n",
        "\n",
        "    # Step 2: Generate region proposals (selective search)\n",
        "    print(\"Generating region proposals...\")\n",
        "    proposals, _ = generate_region_proposals(image_path, scale=500)\n",
        "\n",
        "    # Convert proposals to boxes format\n",
        "    boxes = []\n",
        "    for prop in proposals[:1000]:  # Limit to 1000 proposals\n",
        "        x, y, w, h = prop['rect']\n",
        "        boxes.append([x, y, x+w, y+h])\n",
        "\n",
        "    boxes_tensor = torch.FloatTensor(boxes).to(device)\n",
        "\n",
        "    # Add batch index\n",
        "    batch_indices = torch.zeros(\n",
        "        (boxes_tensor.shape[0], 1),\n",
        "        dtype=boxes_tensor.dtype,\n",
        "        device=boxes_tensor.device\n",
        "    )\n",
        "    rois = torch.cat([batch_indices, boxes_tensor], dim=1)\n",
        "\n",
        "    # Step 3: Forward pass\n",
        "    print(\"Running Fast R-CNN inference...\")\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        cls_scores, bbox_deltas = model(img_tensor, rois)\n",
        "\n",
        "    # Step 4: Post-processing\n",
        "    # Apply softmax to get probabilities\n",
        "    probs = torch.softmax(cls_scores, dim=1)\n",
        "\n",
        "    # Get max class and score for each box\n",
        "    scores, labels = torch.max(probs, dim=1)\n",
        "\n",
        "    # Filter background class (0) and low confidence\n",
        "    keep = (labels > 0) & (scores > threshold)\n",
        "    boxes_filtered = boxes_tensor[keep].cpu().numpy()\n",
        "    labels_filtered = labels[keep].cpu().numpy()\n",
        "    scores_filtered = scores[keep].cpu().numpy()\n",
        "\n",
        "    # Apply NMS\n",
        "    if len(boxes_filtered) > 0:\n",
        "        keep_nms = ops.nms(\n",
        "            torch.FloatTensor(boxes_filtered),\n",
        "            torch.FloatTensor(scores_filtered),\n",
        "            nms_threshold\n",
        "        )\n",
        "        boxes_filtered = boxes_filtered[keep_nms]\n",
        "        labels_filtered = labels_filtered[keep_nms]\n",
        "        scores_filtered = scores_filtered[keep_nms]\n",
        "\n",
        "    print(f\"Found {len(boxes_filtered)} objects after NMS\")\n",
        "\n",
        "    return boxes_filtered, labels_filtered, scores_filtered, img_rgb\n",
        "\n",
        "\n",
        "# Note: For actual inference, we'll use torchvision's pre-trained model\n",
        "# as training Fast R-CNN from scratch requires extensive labeled data\n",
        "\n",
        "print(\"Note: Fast R-CNN training requires labeled dataset\")\n",
        "print(\"For demonstration, we'll use pre-trained Faster R-CNN which includes Fast R-CNN architecture\")\n",
        "\n",
        "# Use pre-trained Faster R-CNN for demonstration\n",
        "model_pretrained = fasterrcnn_resnet50_fpn(weights='DEFAULT')\n",
        "model_pretrained.to(device)\n",
        "model_pretrained.eval()\n",
        "\n",
        "def fast_rcnn_demo(image_path, threshold=0.7):\n",
        "    \"\"\"\n",
        "    Demonstrate Fast R-CNN using pre-trained Faster R-CNN\n",
        "    \"\"\"\n",
        "    # Load image\n",
        "    img = Image.open(image_path).convert('RGB')\n",
        "    img_tensor = transforms.ToTensor()(img).to(device)\n",
        "\n",
        "    # Inference\n",
        "    with torch.no_grad():\n",
        "        predictions = model_pretrained([img_tensor])\n",
        "\n",
        "    # Extract results\n",
        "    boxes = predictions[0]['boxes'].cpu().numpy()\n",
        "    labels = predictions[0]['labels'].cpu().numpy()\n",
        "    scores = predictions[0]['scores'].cpu().numpy()\n",
        "\n",
        "    # Filter by threshold\n",
        "    keep = scores > threshold\n",
        "    boxes = boxes[keep]\n",
        "    labels = labels[keep]\n",
        "    scores = scores[keep]\n",
        "\n",
        "    return boxes, labels, scores, np.array(img)\n",
        "\n",
        "\n",
        "# Run inference\n",
        "boxes, labels, scores, img = fast_rcnn_demo(\n",
        "    'PennFudanPed/PNGImages/FudanPed00005.png',\n",
        "    threshold=0.7\n",
        ")\n",
        "\n",
        "# Visualize\n",
        "fig, ax = plt.subplots(1, figsize=(12, 8))\n",
        "ax.imshow(img)\n",
        "\n",
        "for box, label, score in zip(boxes, labels, scores):\n",
        "    x1, y1, x2, y2 = box\n",
        "    w, h = x2 - x1, y2 - y1\n",
        "    rect = patches.Rectangle(\n",
        "        (x1, y1), w, h, linewidth=2,\n",
        "        edgecolor='blue', facecolor='none'\n",
        "    )\n",
        "    ax.add_patch(rect)\n",
        "\n",
        "    class_name = COCO_CLASSES[label] if label < len(COCO_CLASSES) else f'class_{label}'\n",
        "    ax.text(\n",
        "        x1, y1-5, f'{class_name}: {score:.2f}',\n",
        "        bbox=dict(facecolor='blue', alpha=0.5),\n",
        "        fontsize=10, color='white'\n",
        "    )\n",
        "\n",
        "ax.set_title('Fast R-CNN Detection Results')\n",
        "plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.savefig('fast_rcnn_results.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nFast R-CNN Detection Summary:\")\n",
        "print(f\"Total detections: {len(boxes)}\")\n",
        "for i, (label, score) in enumerate(zip(labels, scores)):\n",
        "    class_name = COCO_CLASSES[label] if label < len(COCO_CLASSES) else f'class_{label}'\n",
        "    print(f\"  {i+1}. {class_name}: {score:.3f}\")"
      ],
      "metadata": {
        "id": "RqYgwSuzgUbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Langkah 2.4: Perbandingan R-CNN vs Fast R-CNN"
      ],
      "metadata": {
        "id": "ztTu4eDldbI9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def compare_rcnn_vs_fast_rcnn(image_path):\n",
        "    \"\"\"\n",
        "    Compare inference time between R-CNN and Fast R-CNN\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "    print(\"Comparing R-CNN vs Fast R-CNN\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # R-CNN (simplified version)\n",
        "    print(\"\\n[R-CNN]\")\n",
        "    start_time = time.time()\n",
        "    boxes_rcnn, labels_rcnn, scores_rcnn, _ = rcnn_inference_simplified(\n",
        "        image_path, threshold=0.7\n",
        "    )\n",
        "    rcnn_time = time.time() - start_time\n",
        "    print(f\"Inference time: {rcnn_time:.2f} seconds\")\n",
        "    print(f\"Detections: {len(boxes_rcnn)}\")\n",
        "\n",
        "    # Fast R-CNN\n",
        "    print(\"\\n[Fast R-CNN]\")\n",
        "    start_time = time.time()\n",
        "    boxes_fast, labels_fast, scores_fast, _ = fast_rcnn_demo(\n",
        "        image_path, threshold=0.7\n",
        "    )\n",
        "    fast_rcnn_time = time.time() - start_time\n",
        "    print(f\"Inference time: {fast_rcnn_time:.2f} seconds\")\n",
        "    print(f\"Detections: {len(boxes_fast)}\")\n",
        "\n",
        "    # Summary\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"SUMMARY\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"R-CNN time:      {rcnn_time:.2f}s\")\n",
        "    print(f\"Fast R-CNN time: {fast_rcnn_time:.2f}s\")\n",
        "    print(f\"Speedup:         {rcnn_time/fast_rcnn_time:.2f}x faster\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "\n",
        "# Run comparison\n",
        "compare_rcnn_vs_fast_rcnn('PennFudanPed/PNGImages/FudanPed00010.png')"
      ],
      "metadata": {
        "id": "va5PnSd7gcQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Praktikum 3: Implementasi Faster R-CNN"
      ],
      "metadata": {
        "id": "ar1xeIDGdgwe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Langkah 3.1: Load Pre-trained Faster R-CNN"
      ],
      "metadata": {
        "id": "3evAj_UodlpP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn, \\\n",
        "    FasterRCNN_ResNet50_FPN_Weights\n",
        "\n",
        "# Load pre-trained Faster R-CNN\n",
        "weights = FasterRCNN_ResNet50_FPN_Weights.DEFAULT\n",
        "model_faster_rcnn = fasterrcnn_resnet50_fpn(weights=weights)\n",
        "model_faster_rcnn.to(device)\n",
        "model_faster_rcnn.eval()\n",
        "\n",
        "print(\"Faster R-CNN Model Loaded\")\n",
        "print(f\"Model trained on: COCO dataset (80 classes)\")\n",
        "print(f\"Backbone: ResNet-50 with FPN (Feature Pyramid Network)\")\n",
        "\n",
        "# COCO class names\n",
        "COCO_INSTANCE_CATEGORIES = [\n",
        "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane',\n",
        "    'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A',\n",
        "    'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse',\n",
        "    'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack',\n",
        "    'umbrella', 'N/A', 'N/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis',\n",
        "    'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',\n",
        "    'skateboard', 'surfboard', 'tennis racket', 'bottle', 'N/A', 'wine glass',\n",
        "    'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich',\n",
        "    'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake',\n",
        "    'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table', 'N/A',\n",
        "    'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard',\n",
        "    'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator',\n",
        "    'N/A', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier',\n",
        "    'toothbrush'\n",
        "]"
      ],
      "metadata": {
        "id": "Uhu6WNiOgyhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Langkah 3.2: Implementasi Inference Function"
      ],
      "metadata": {
        "id": "Y9yUHbE1drct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def faster_rcnn_inference(image_path, model, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Faster R-CNN inference pipeline\n",
        "\n",
        "    Args:\n",
        "        image_path: Path to input image\n",
        "        model: Faster R-CNN model\n",
        "        threshold: Confidence threshold\n",
        "\n",
        "    Returns:\n",
        "        boxes, labels, scores, image: Detection results\n",
        "    \"\"\"\n",
        "\n",
        "    # Load image\n",
        "    img = Image.open(image_path).convert('RGB')\n",
        "    img_tensor = transforms.ToTensor()(img).to(device)\n",
        "\n",
        "    # Inference\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        start_time = time.time()\n",
        "        predictions = model([img_tensor])\n",
        "        inference_time = time.time() - start_time\n",
        "\n",
        "    # Extract results\n",
        "    boxes = predictions[0]['boxes'].cpu().numpy()\n",
        "    labels = predictions[0]['labels'].cpu().numpy()\n",
        "    scores = predictions[0]['scores'].cpu().numpy()\n",
        "\n",
        "    # Filter by threshold\n",
        "    keep = scores > threshold\n",
        "    boxes = boxes[keep]\n",
        "    labels = labels[keep]\n",
        "    scores = scores[keep]\n",
        "\n",
        "    print(f\"Inference time: {inference_time:.3f} seconds\")\n",
        "    print(f\"Detected {len(boxes)} objects above threshold {threshold}\")\n",
        "\n",
        "    return boxes, labels, scores, np.array(img), inference_time\n",
        "\n",
        "\n",
        "# Run inference\n",
        "boxes, labels, scores, img, inf_time = faster_rcnn_inference(\n",
        "    'PennFudanPed/PNGImages/FudanPed00015.png',\n",
        "    model_faster_rcnn,\n",
        "    threshold=0.7\n",
        ")\n",
        "\n",
        "# Visualize results\n",
        "fig, ax = plt.subplots(1, figsize=(12, 8))\n",
        "ax.imshow(img)\n",
        "\n",
        "colors = ['red', 'blue', 'green', 'yellow', 'orange', 'purple']\n",
        "for i, (box, label, score) in enumerate(zip(boxes, labels, scores)):\n",
        "    x1, y1, x2, y2 = box\n",
        "    w, h = x2 - x1, y2 - y1\n",
        "    color = colors[i % len(colors)]\n",
        "    rect = patches.Rectangle(\n",
        "        (x1, y1), w, h, linewidth=3,\n",
        "        edgecolor=color, facecolor='none'\n",
        "    )\n",
        "    ax.add_patch(rect)\n",
        "\n",
        "    class_name = COCO_INSTANCE_CATEGORIES[label]\n",
        "    ax.text(\n",
        "        x1, y1-10, f'{class_name}: {score:.2f}',\n",
        "        bbox=dict(facecolor=color, alpha=0.7),\n",
        "        fontsize=12, color='white', weight='bold'\n",
        "    )\n",
        "\n",
        "ax.set_title(\n",
        "    f'Faster R-CNN Detection Results (Inference: {inf_time:.3f}s)',\n",
        "    fontsize=14, weight='bold'\n",
        ")\n",
        "plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.savefig('faster_rcnn_results.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Print detection details\n",
        "print(f\"\\nDetection Details:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for i, (box, label, score) in enumerate(zip(boxes, labels, scores)):\n",
        "    class_name = COCO_INSTANCE_CATEGORIES[label]\n",
        "    x1, y1, x2, y2 = box\n",
        "    print(f\"{i+1}. {class_name:15s} | Score: {score:.3f} | Box: [{x1:.0f}, {y1:.0f}, {x2:.0f}, {y2:.0f}]\")\n",
        "\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {
        "id": "Jf53UCaOg60f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Langkah 3.3: Batch Inference untuk Multiple Image"
      ],
      "metadata": {
        "id": "WuifH70xdwGN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def faster_rcnn_batch_inference(image_paths, model, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Batch inference for multiple images\n",
        "\n",
        "    Args:\n",
        "        image_paths: List of image paths\n",
        "        model: Faster R-CNN model\n",
        "        threshold: Confidence threshold\n",
        "\n",
        "    Returns:\n",
        "        results: List of detection results for each image\n",
        "    \"\"\"\n",
        "\n",
        "    results = []\n",
        "    total_time = 0\n",
        "    model.eval()\n",
        "\n",
        "    for i, img_path in enumerate(image_paths):\n",
        "        print(f\"\\nProcessing image {i+1}/{len(image_paths)}: {os.path.basename(img_path)}\")\n",
        "\n",
        "        # Load image\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        img_tensor = transforms.ToTensor()(img).to(device)\n",
        "\n",
        "        # Inference\n",
        "        with torch.no_grad():\n",
        "            start_time = time.time()\n",
        "            predictions = model([img_tensor])\n",
        "            inference_time = time.time() - start_time\n",
        "            total_time += inference_time\n",
        "\n",
        "        # Extract results\n",
        "        boxes = predictions[0]['boxes'].cpu().numpy()\n",
        "        labels = predictions[0]['labels'].cpu().numpy()\n",
        "        scores = predictions[0]['scores'].cpu().numpy()\n",
        "\n",
        "        # Filter by threshold\n",
        "        keep = scores > threshold\n",
        "        boxes = boxes[keep]\n",
        "        labels = labels[keep]\n",
        "        scores = scores[keep]\n",
        "\n",
        "        print(f\"  Inference time: {inference_time:.3f}s | Detections: {len(boxes)}\")\n",
        "\n",
        "        results.append({\n",
        "            'image_path': img_path,\n",
        "            'boxes': boxes,\n",
        "            'labels': labels,\n",
        "            'scores': scores,\n",
        "            'image': np.array(img),\n",
        "            'inference_time': inference_time\n",
        "        })\n",
        "\n",
        "    avg_time = total_time / len(image_paths)\n",
        "    fps = 1.0 / avg_time\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"Batch Inference Summary\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Total images:     {len(image_paths)}\")\n",
        "    print(f\"Total time:       {total_time:.3f}s\")\n",
        "    print(f\"Average time:     {avg_time:.3f}s per image\")\n",
        "    print(f\"Throughput:       {fps:.2f} FPS\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# Get sample images\n",
        "sample_images = [\n",
        "    'PennFudanPed/PNGImages/FudanPed00001.png',\n",
        "    'PennFudanPed/PNGImages/FudanPed00005.png',\n",
        "    'PennFudanPed/PNGImages/FudanPed00010.png',\n",
        "    'PennFudanPed/PNGImages/FudanPed00015.png',\n",
        "    'PennFudanPed/PNGImages/FudanPed00020.png',\n",
        "]\n",
        "\n",
        "# Run batch inference\n",
        "batch_results = faster_rcnn_batch_inference(\n",
        "    sample_images,\n",
        "    model_faster_rcnn,\n",
        "    threshold=0.7\n",
        ")\n",
        "\n",
        "# Visualize all results\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, result in enumerate(batch_results):\n",
        "    if idx >= 6:\n",
        "        break\n",
        "\n",
        "    ax = axes[idx]\n",
        "    img = result['image']\n",
        "    boxes = result['boxes']\n",
        "    labels = result['labels']\n",
        "    scores = result['scores']\n",
        "\n",
        "    ax.imshow(img)\n",
        "\n",
        "    for box, label, score in zip(boxes, labels, scores):\n",
        "        x1, y1, x2, y2 = box\n",
        "        w, h = x2 - x1, y2 - y1\n",
        "\n",
        "        rect = patches.Rectangle(\n",
        "            (x1, y1), w, h, linewidth=2,\n",
        "            edgecolor='green', facecolor='none'\n",
        "        )\n",
        "        ax.add_patch(rect)\n",
        "\n",
        "        class_name = COCO_INSTANCE_CATEGORIES[label]\n",
        "        ax.text(\n",
        "            x1, y1-5, f'{class_name}: {score:.2f}',\n",
        "            bbox=dict(facecolor='green', alpha=0.5),\n",
        "            fontsize=8, color='white'\n",
        "        )\n",
        "\n",
        "    ax.set_title(f\"Image {idx+1} ({result['inference_time']:.3f}s)\", fontsize=10)\n",
        "    ax.axis('off')\n",
        "\n",
        "# Hide unused subplot\n",
        "if len(batch_results) < 6:\n",
        "    axes[5].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('faster_rcnn_batch_results.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "f45AJas4g8JV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Langkah 3.4: Fine-tuning Faster R-CNN pada Custom Dataset"
      ],
      "metadata": {
        "id": "bUtsp71Md2vg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.utils.data as data\n",
        "from torchvision.ops.boxes import masks_to_boxes\n",
        "from torchvision import tv_tensors\n",
        "\n",
        "class PennFudanDataset(data.Dataset):\n",
        "    \"\"\"\n",
        "    Custom dataset for Penn-Fudan Pedestrian Detection\n",
        "    \"\"\"\n",
        "    def __init__(self, root, transforms=None):\n",
        "        self.root = root\n",
        "        self.transforms = transforms\n",
        "\n",
        "        # Load all image files\n",
        "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
        "        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load image and mask\n",
        "        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
        "        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
        "\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        mask = Image.open(mask_path)\n",
        "\n",
        "        # Convert to numpy\n",
        "        mask = np.array(mask)\n",
        "\n",
        "        # Instances are encoded as different colors\n",
        "        obj_ids = np.unique(mask)\n",
        "        # Remove background\n",
        "        obj_ids = obj_ids[1:]\n",
        "\n",
        "        # Split mask into binary masks\n",
        "        masks = mask == obj_ids[:, None, None]\n",
        "\n",
        "        # Get bounding boxes\n",
        "        num_objs = len(obj_ids)\n",
        "        boxes = []\n",
        "        for i in range(num_objs):\n",
        "            pos = np.where(masks[i])\n",
        "            xmin = np.min(pos[1])\n",
        "            xmax = np.max(pos[1])\n",
        "            ymin = np.min(pos[0])\n",
        "            ymax = np.max(pos[0])\n",
        "            boxes.append([xmin, ymin, xmax, ymax])\n",
        "\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.ones((num_objs,), dtype=torch.int64)  # Only one class: person\n",
        "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
        "\n",
        "        image_id = torch.tensor([idx])\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"masks\"] = masks\n",
        "        target[\"image_id\"] = image_id\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            img, target = self.transforms(img, target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "# Create dataset\n",
        "dataset = PennFudanDataset('PennFudanPed')\n",
        "print(f\"Dataset size: {len(dataset)}\")\n",
        "\n",
        "# Test dataset\n",
        "img, target = dataset[0]\n",
        "print(f\"Image type: {type(img)}\")\n",
        "print(f\"Target keys: {target.keys()}\")\n",
        "print(f\"Number of objects: {len(target['boxes'])}\")\n",
        "print(f\"Boxes shape: {target['boxes'].shape}\")\n",
        "print(f\"Labels: {target['labels']}\")"
      ],
      "metadata": {
        "id": "OLoh_LEthVVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Langkah 3.5: Training Loop untuk Fine-tuning"
      ],
      "metadata": {
        "id": "aiXS5IV5d9dW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_for_finetuning(num_classes):\n",
        "    \"\"\"\n",
        "    Get Faster R-CNN model for fine-tuning\n",
        "\n",
        "    Args:\n",
        "        num_classes: Number of classes (including background)\n",
        "\n",
        "    Returns:\n",
        "        model: Faster R-CNN model\n",
        "    \"\"\"\n",
        "    # Load pre-trained model\n",
        "    model = fasterrcnn_resnet50_fpn(weights='DEFAULT')\n",
        "\n",
        "    # Get number of input features for the classifier\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "\n",
        "    # Replace the pre-trained head with a new one\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def train_one_epoch(model, optimizer, data_loader, device, epoch):\n",
        "    \"\"\"\n",
        "    Train for one epoch\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for i, (images, targets) in enumerate(data_loader):\n",
        "        images = list(image.to(device) for image in images)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        # Forward pass\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += losses.item()\n",
        "\n",
        "        if (i + 1) % 10 == 0:\n",
        "            print(f\"Epoch [{epoch}], Step [{i+1}/{len(data_loader)}], Loss: {losses.item():.4f}\")\n",
        "\n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "    print(f\"Epoch [{epoch}] Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    return avg_loss\n",
        "\n",
        "\n",
        "# Prepare for training\n",
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "# Split dataset\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "print(f\"Train size: {len(train_dataset)}\")\n",
        "print(f\"Validation size: {len(val_dataset)}\")\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=2,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "val_loader = data.DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=2,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "# Initialize model for fine-tuning\n",
        "num_classes = 2  # 1 class (person) + background\n",
        "model_finetuned = get_model_for_finetuning(num_classes)\n",
        "model_finetuned.to(device)\n",
        "\n",
        "# Setup optimizer\n",
        "params = [p for p in model_finetuned.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
        "\n",
        "# Learning rate scheduler\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "\n",
        "# Training loop (demo with 2 epochs)\n",
        "num_epochs = 2\n",
        "print(f\"\\nStarting training for {num_epochs} epochs...\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "training_losses = []\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    print(f\"\\nEpoch {epoch}/{num_epochs}\")\n",
        "    print(f\"{'-'*60}\")\n",
        "\n",
        "    # Train\n",
        "    avg_loss = train_one_epoch(model_finetuned, optimizer, train_loader, device, epoch)\n",
        "    training_losses.append(avg_loss)\n",
        "\n",
        "    # Update learning rate\n",
        "    lr_scheduler.step()\n",
        "    print(f\"Current learning rate: {optimizer.param_groups[0]['lr']}\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"Training completed!\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "# Plot training loss\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, num_epochs + 1), training_losses, marker='o', linewidth=2, markersize=8)\n",
        "plt.xlabel('Epoch', fontsize=12)\n",
        "plt.ylabel('Average Loss', fontsize=12)\n",
        "plt.title('Training Loss over Epochs', fontsize=14, weight='bold')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('training_loss.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Save model\n",
        "torch.save(model_finetuned.state_dict(), 'faster_rcnn_finetuned.pth')\n",
        "print(\"\\nModel saved as 'faster_rcnn_finetuned.pth'\")"
      ],
      "metadata": {
        "id": "HgvFAAFvhheH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Langkah 3.6: Evaluasi Model Fine-tuning"
      ],
      "metadata": {
        "id": "g6Mp6UVzeCt2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, data_loader, device):\n",
        "    \"\"\"\n",
        "    Evaluate model on validation set\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, targets in data_loader:\n",
        "            images = list(image.to(device) for image in images)\n",
        "            predictions = model(images)\n",
        "\n",
        "            all_predictions.extend(predictions)\n",
        "            all_targets.extend(targets)\n",
        "\n",
        "    return all_predictions, all_targets\n",
        "\n",
        "\n",
        "# Evaluate fine-tuned model\n",
        "print(\"Evaluating fine-tuned model...\")\n",
        "predictions, targets = evaluate_model(model_finetuned, val_loader, device)\n",
        "\n",
        "# Visualize predictions on validation set\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx in range(min(4, len(val_dataset))):\n",
        "    img, target = val_dataset[idx]\n",
        "\n",
        "    # Get prediction\n",
        "    img_tensor = img.to(device)\n",
        "    with torch.no_grad():\n",
        "        prediction = model_finetuned([img_tensor])[0]\n",
        "\n",
        "    # Convert image to numpy\n",
        "    img_np = img.permute(1, 2, 0).cpu().numpy()\n",
        "\n",
        "    # Plot\n",
        "    ax = axes[idx]\n",
        "    ax.imshow(img_np)\n",
        "\n",
        "    # Plot ground truth (green)\n",
        "    gt_boxes = target['boxes'].cpu().numpy()\n",
        "    for box in gt_boxes:\n",
        "        x1, y1, x2, y2 = box\n",
        "        w, h = x2 - x1, y2 - y1\n",
        "        rect = patches.Rectangle(\n",
        "            (x1, y1), w, h, linewidth=2,\n",
        "            edgecolor='green', facecolor='none',\n",
        "            label='Ground Truth'\n",
        "        )\n",
        "        ax.add_patch(rect)\n",
        "\n",
        "    # Plot predictions (red)\n",
        "pred_boxes = prediction['boxes'].cpu().numpy()\n",
        "pred_scores = prediction['scores'].cpu().numpy()\n",
        "\n",
        "for box, score in zip(pred_boxes, pred_scores):\n",
        "    if score > 0.5:  # Threshold\n",
        "        x1, y1, x2, y2 = box\n",
        "        w, h = x2 - x1, y2 - y1\n",
        "        rect = patches.Rectangle(\n",
        "            (x1, y1), w, h, linewidth=2,\n",
        "            edgecolor='red', facecolor='none',\n",
        "            linestyle='--', label='Prediction'\n",
        "        )\n",
        "        ax.add_patch(rect)\n",
        "        ax.text(\n",
        "            x1, y1-5, f'{score:.2f}',\n",
        "            bbox=dict(facecolor='red', alpha=0.5),\n",
        "            fontsize=8, color='white'\n",
        "        )\n",
        "\n",
        "ax.set_title(f'Validation Image {idx+1}', fontsize=12)\n",
        "ax.axis('off')\n",
        "\n",
        "# Add legend\n",
        "handles, labels = axes[0].get_legend_handles_labels()\n",
        "if handles:\n",
        "    fig.legend(\n",
        "        handles[:2], ['Ground Truth', 'Prediction'],\n",
        "        loc='upper center', ncol=2, fontsize=12\n",
        "    )\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('finetuned_evaluation.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nEvaluation completed!\")\n",
        "print(f\"Predictions saved to 'finetuned_evaluation.png'\")"
      ],
      "metadata": {
        "id": "WWi_MntShtkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualisasi Perbandingan Kecepatan"
      ],
      "metadata": {
        "id": "8jPZbIpdeO9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create comparison chart\n",
        "models = ['R-CNN', 'Fast R-CNN', 'Faster R-CNN']\n",
        "inference_times_cpu = [47.0, 2.3, 2.0]  # seconds\n",
        "inference_times_gpu = [47.0, 0.32, 0.2]  # seconds\n",
        "map_scores = [66.0, 68.4, 73.2]  # mAP on PASCAL VOC 2007\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Inference time comparison\n",
        "x = np.arange(len(models))\n",
        "width = 0.35\n",
        "\n",
        "bars1 = ax1.bar(x - width/2, inference_times_cpu, width, label='CPU',\n",
        "                color='steelblue')\n",
        "bars2 = ax1.bar(x + width/2, inference_times_gpu, width, label='GPU',\n",
        "                color='coral')\n",
        "\n",
        "ax1.set_ylabel('Inference Time (seconds)', fontsize=12, weight='bold')\n",
        "ax1.set_title('Inference Time Comparison', fontsize=14, weight='bold')\n",
        "ax1.set_xticks(x)\n",
        "ax1.set_xticklabels(models)\n",
        "ax1.legend(fontsize=11)\n",
        "ax1.grid(axis='y', alpha=0.3)\n",
        "ax1.set_yscale('log')\n",
        "\n",
        "# Add value labels on bars\n",
        "for bars in [bars1, bars2]:\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        ax1.text(\n",
        "            bar.get_x() + bar.get_width()/2., height,\n",
        "            f'{height:.2f}s',\n",
        "            ha='center', va='bottom', fontsize=9\n",
        "        )\n",
        "\n",
        "# mAP comparison\n",
        "bars3 = ax2.bar(models, map_scores,\n",
        "                color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
        "\n",
        "ax2.set_ylabel('mAP (%)', fontsize=12, weight='bold')\n",
        "ax2.set_title('Accuracy Comparison (PASCAL VOC 2007)', fontsize=14,\n",
        "              weight='bold')\n",
        "ax2.set_ylim([60, 75])\n",
        "ax2.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Add value labels\n",
        "for i, (model, score) in enumerate(zip(models, map_scores)):\n",
        "    ax2.text(\n",
        "        i, score + 0.5, f'{score:.1f}%',\n",
        "        ha='center', va='bottom', fontsize=11, weight='bold'\n",
        "    )\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('model_comparison.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nPerformance Comparison Summary:\")\n",
        "print(\"=\"*70)\n",
        "print(f\"{'Model':<20} {'CPU Time':<15} {'GPU Time':<15} {'mAP':<10}\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for model, cpu_time, gpu_time, map_val in zip(models, inference_times_cpu,\n",
        "                                             inference_times_gpu,\n",
        "                                             map_scores):\n",
        "    print(f\"{model:<20} {cpu_time:>10.2f}s     {gpu_time:>10.3f}s     {map_val:>6.1f}%\")\n",
        "\n",
        "print(\"=\"*70)\n"
      ],
      "metadata": {
        "id": "C8JfWACvcz3O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}